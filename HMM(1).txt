隐式马尔科夫（HMM）的简单介绍
1）隐式马尔科夫的定义
	状态：Q=q[1],q[2],...,q[N] （ASR中状态就是音素，Part-of-speech tagging中状态就是语法标志）
	状态转移矩阵：A=a[1][1],a[1][2],...,a[N][1],...,a[N][N] 
	观测向量：O=o[1],o[2],...,o[T] 
		观测向量里的每一个观测向量都属于单词表V={v1,v2,...,vk,...,vn}
		单词表相当于离散化向量空间，关于连续的观测向量（类似MFCC）后面会另作讨论。
	发生概率：B=b[i](o[t])
		状态q[i]发生观测o[t]的概率是b[i](o[t])
	开始状态和终结状态的两种表示方式：
		1、开始状态q[0]，和终结状态q[F]
		2、初始状态概率分布和合法接受状态
			初始状态概率分布：pai=pai[1],pai[2],pai[3],...,pai[N]
			合法接受状态是状态Q的子集
	理论上确定一个隐式马尔科夫只需要确定A和B就好所以下面把HMM表示成：
		HMM: lamda=(A,B)
2）观测序列相似度得分（前向算法）
	输入：lamda=(A,B)和观测序列O=o[1],o[2],...,o[T]（T是帧数）
	输出：得分（就是这个观测序列属于lamda的概率）
	观测序列和lamda的相似程度就是对应所有可能的状态序列的概率的全加，但是实践中将所有状态序列全部列出来计算过于暴力，所以有了前向(t-1)算法，前向算法属于动态规划。
	第t帧观测向量属于状态i转到j的得分=第t-1帧观测向量属于状态i的得分*a[i][j]*b[j](o[t])
	设：第t-1帧观测向量属于状态j的得分=alpha[t-1](i)
	第t帧观测向量属于状态j的得分就是所有状态转到状态j的得分全加，于是有：
	alpha[t](j)=alpha[t-1](1)*a[1][j]*b[j](o[t])+...+alpha[t-1](i)*a[i][j]*b[j](o[t])+...+alpha[t-1](N)*a[N][j]*b[j](o[t])
	上面的式子从2迭代到T
	初始化：alpha[1](j)=初始状态概率*初始状态概率转状态j的概率*b[j](o[1])
	终结态：最终得分=alpha[T](1)*a[1][F]+...+alpha[T](s)*a[s][F]+...+alpha[T](N)*a[N][F]  其中，a[s][F]代表状态s转终末状态F的概率。
3）观测序列解码（Viterbi算法）
	输入：lamda=(A,B)和观测序列O=o[1],o[2],...,o[T]（T是帧数）
	输出：最优状态序列Q=q[1],q[2],...,q[T]（以及这个最优状态序列的概率）
	所谓解码就是把所有的状态序列全部列一遍然后算每种状态序列的得分然后取得分最大的那个序列，但是这样太暴力，于是就有了Viterbi算法，Viterbi算法属于动态规划算法。
	设：v[t](j)是t时刻是j状态时最佳状态序列的概率
	所以：v[t](j)=max( v[t-1](1)*a[1][j]*b[j](o[t]),...,v[t-1](i)*a[i][j]*b[j](o[t]),...,v[t-1](N)*a[N][j]*b[j](o[t]) )
	上面的式子从1迭代到T，并记录每一个t对应的j。
	初始化：v[1](j)=初始状态概率*初始状态概率转状态j的概率*b[j](o[1])
	终结态：最终得分=max( v[T](1)*a[1][F]),...,v[T](s)*a[s][F],...,v[T](N)*a[N][F] )  其中，a[s][F]代表状态s转终末状态F的概率
	Viterbi和前向算法是非常相近的，不同点在于前向做Sum Viterbi做Max，还有Viterbi要记录状态序列。
4）训练（前向后向算法）
	输入：观测序列集合、状态集合
	输出：lamda=(A,B)
	4.1 下面先计算A：
		根据A的定义有：
			公式01：_a[i][j]=(从状态i转状态j的次数)/(从状态i出发转状态的次数)
		为了从观测序列中统计从状态i转状态j的次数，定义sigma
		sigma[t](i,j)是当观测序列O和lamda作为条件时，t时刻是状态i并且t+1时刻是状态j的概率，也就是下面式子：
			公式02：sigma[t](i,j)=P(q[t]=i,q[t+1]=j | O,lamda)
		观察到O依旧在概率的条件里，所以要把O放出来。下面用一次条件概率就会有：
			公式03：P(q[t]=i,q[t+1]=j | O,lamda)=P(q[t]=i,q[t+1]=j, O| lamda)/P(O|lamda)
		P(q[t]=i,q[t+1]=j, O| lamda)表示lamda条件下观测序列O中，t时刻是状态i并且t+1时刻是状态j的概率
		P(O|lamda)表示lamda发生观测序列O的概率，这里lamda发生观测序列O的概率就是用前向算法算出的相似度得分，也就是：alpha[T](F)
		为了计算lamda条件下i->j在观测序列O中发生的概率，我们需要引入后向概率，计算后向概率的方法叫后向算法（因为前向算法只覆盖到1...t-1部分，所以需要t+1...T才能覆盖住整个O）
		后向概率：
			公式04：beta[t](i)=sum(a[i][1]*b[1](o[t+1])*beta[t+1](1),...,a[i][j]*b[j](o[t+1])*beta[t+1](j),...,a[i][N]*b[N](o[t+1])*beta[t+1](N))
		然后我们就有：
			公式05：P(q[t]=i,q[t+1]=j, O| lamda)=alpha[t](i)*a[i][j]*b[j](o[t+1])*beta[t+1](j)
		带入公式3就可以有：
			公式06：sigma[t](i,j)=( alpha[t](i)*a[i][j]*b[j](o[t+1])*beta[t+1](j) )/(alpha[T](F))
		下面统计从状态i转状态j的次数：
		设从状态i转状态j的次数为SumT[i][j]
			公式07：SumT[i][j]=sum(sigma[1](i,j),   ,sigma[t](i,j),   ,sigma[T-1](i,j))
		下面统计从状态i出发转状态的次数:
			公式08：从状态i出发转状态的次数=sum(SumT[i][1],...,SumT[i][j],...,SumT[i][N])
		把公式7和公式8带入公式1：
			公式09：_a[i][j]=SumT[i][j]/(SumT[i][1],...,SumT[i][j],...,SumT[i][N])
	4.2 下面计算B（离散单词表）:
		设vk属于V
		那么根据B的定义有：
			公式10：_b[j](vk)=( 观测值是vk并且状态是j的次数 )/( 状态j的次数 )
		为了计算状态j出现的次数，我们需要计算每一个时刻状态j出现的概率，也就是下面的公式：
			公式11：gamma[t](j)=P(q[t]=j | O,lamda)
		也就是lamda产生观测序列O时，时刻t是状态j的概率是gamma[t](j)
		观察到O依旧在概率的条件里，所以要把O放出来。下面用一次条件概率就会有：
			公式12：gamma[t](j)=P(q[t]=j,O|lamda)/P(O|lamda)
		其中P(O|lamda)就是一次前向概率。
		P(q[t]=j,O|lamda)表示由lamda产生的序列O中t时刻发生状态j的概率。t时刻发生状态j的条件是需要同时满足t时刻之前的序列会发生j和t时刻之后的序列需要j发生，所以：
			公式13：P(q[t]=j,O|lamda)=alpha[t](j)*beta[t](j)
		然后把公式13带入到公式12
			公式14：gamma[t](j)=alpha[t](j)*beta[t](j)/P(O|lamda)
		于是状态j的次数SumGamma就可以表示成：
			公式15：SumGamma=sum(gamma[1](j),...,gamma[t](j),...,gamma[T](j))
		观测值是vk并且状态是j的次数就是把观测值是vk的gamma[t](j)全加，下面写成：
			公式16：观测值是vk并且状态是j的次数=sum[o[t]=vk](gamma[1](j),...,gamma[t](j),...,gamma[T](j))
		然后把公式16和15带入公式10：
			公式17：_b[j](vk)=sum[o[t]=vk](gamma[1](j),...,gamma[t](j),...,gamma[T](j)) / sum(gamma[1](j),...,gamma[t](j),...,gamma[T](j))
	4.3 前向后向算法流程：
		输入：观测序列O 单词表V 状态集合Q
		输出：lamda=(A,B)
		初始化A和B
		迭代直到收敛
			E-step：
				计算公式6
				计算公式14
			M-step:
				计算公式9
				计算公式17
			A=_A
			B=_B
		return A,B
	下面解决连续观测向量的计算问题：
	4.4 Vector Quantization（VQ）
		对于实值向量来说（例如MFCC），公式16的计算中o[t]是否等于vk的操作并不可行。所以需要在实值向量和单词表之间做一个映射表，这个过程就是VQ，这个映射表就叫码本。
		VQ的方法用kmeans（这里不再介绍kmeans）
		然后把公式16里的o[t]=vk解释成vk是码本中最接近o[t]的单词。
	4.5 Gaussian
		用VQ损失太大了，无法适应高级任务。
		在VQ法中我们做一个码本从实值向量转到单词表然后执行4.2的内容，直接面对实值向量训练b[i](o[t])的话，那么4.2的方法中“观测值是vk并且状态是j的次数”是无法统计的。
		于是我们重新考察b[i](o[t])的意义，于是我们有：
			公式18：b[i](o[t])=P(o[t]|q[i])
		其中P(o[t]|q[i])代表状态q[i]发生观测o[t]的概率，现在我们假设o[t]服从正太分布：
			o[t]~N(mu,var)
		实际上就是：
			公式19：b[i](o[t])=N(mu,var)
		其中mu就是所有q[t]=i时，o的均值
		其中var就是所有q[t]=i时，o的方差（多维时就是方差矩阵）
		但是实际上HMM并不确定t时刻到底是什么状态，只有一个t时刻是某状态的概率也就是公式14。
		所以直接用o[t]和gamma[t](j)相乘来解决这个问题，然后就有：
			公式20：mu[i]=sum(gamma[1](i)*o[1],...,gamma[t](i)*o[t],...,gamma[T](i)*o[T])/SumGamma
			公式21：var[i]=sum(gamma[1](i)*(o[1]-mu[i])^2,...,gamma[t](i)*(o[t]-mu[i])^2,...,gamma[T](i)*(o[T]-mu[i])^2)/SumGamma
		多元高斯不再赘述。
		上面4.3中公式17替换成公式20和公式21，然后B不再是一个矩阵而是N个高斯函数。
	4.6 Gaussian Mixture Model(GMM)
		观测向量的分布并不一定服从正太分布，所以考虑用多个正太分布近似真实分布。
		设混合高斯分布有M项高斯,c[k]表示第k个高斯的权重。B表示成：
			公式22：b[j](o[t])=sum(c[j][1]*b[j][1](o[t]),...,c[j][m]*b[j][m](o[t]),...c[j][M]*b[j][M](o[t]))
		现在的困难是假设已知o[t]属于状态i，但是未知o[t]属于GMM中的第几个高斯。
		假设t时刻序列O在lamda上o[t]是状态j的第m个高斯的概率是seata[t][m](j)
			公式23：seata[t][m](i,j)=alpha[t-1](i)*a[i][j]*c[j][m]*b[j][m](o[t])*beta[t](j)
			公式24：seata[t][m](j)=sum(seata[t][m](1,j),...,seata[t][m](i,j),..,seata[t][m](N,j))/P(O|lamda)
		然后就可以借助seata计算混合高斯：
			公式25：S(i)=sum[t=1..T](sum[m=1..M](seata[t][m](i)))
			公式26：mu[i][m]=sum(seata[1][m](i)*o[1],...,seata[t][m](i)*o[t],...,seata[T][m](i)*o[T])/S(i)
			公式27：c[i][m]=sum(seata[1][m](i),...,seata[t][m](i),...,seata[T][m](i))/S(i)
			公式28：var[i][m]=sum(seata[1][m](i)*(o[1]-mu[i][m])^2,...,seata[t][m](i)*(o[t]-mu[i][m])^2,...,seata[T][m](i)*(o[T]-mu[i][m])^2)/S(i)
		下面重写4.3的算法流程：
			前向后向算法流程：
			输入：观测序列O 状态集合Q
			输出：lamda=(A,GMM) GMM=(mu,c,var)
			初始化A和GMM
			迭代直到收敛
				E-step：
					计算公式6
					计算公式24
				M-step:
					计算公式9
					计算公式26
					计算公式27
					计算公式28
			return A,GMM
5）例子
	（to be continued）
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	