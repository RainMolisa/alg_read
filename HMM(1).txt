隐式马尔科夫（HMM）的简单介绍
1）隐式马尔科夫的定义
	状态：Q=q[1],q[2],...,q[N] （ASR中状态就是音素，Part-of-speech tagging中状态就是语法标志）
	状态转移矩阵：A=a[1][1],a[1][2],...,a[N][1],...,a[N][N] （实践中初始值是手动设置的）
	观测向量：O=o[1],o[2],...,o[T] (这里暂时离散化为T种，ASR中观测向量就是MFCC，Part-of-speech tagging中状态就是单词)
	发生概率：B=b[i](o[t]) （这里B用的最多的是混合高斯，用SVM和多层感知的也有很多）
		状态q[i]发生观测o[t]的概率是b[i](o[t])
	开始状态和终结状态的两种表示方式：
		1、开始状态q[0]，和终结状态q[F]
		2、初始状态概率分布和合法接受状态
			初始状态概率分布：pai=pai[1],pai[2],pai[3],...,pai[N]
			合法接受状态是状态Q的子集
	理论上确定一个隐式马尔科夫只需要确定A和B就好所以下面把HMM表示成：
		HMM: lamda=(A,B)
2）观测序列相似度得分（前向算法）
	输入：lamda=(A,B)和观测序列O=o[1],o[2],...,o[T]（T是帧数）
	输出：得分
	观测序列和lamda的相似程度就是对应所有可能的状态序列的概率的全加，但是实践中将所有状态序列全部列出来计算过于暴力，所以有了前向(t-1)算法，前向算法属于动态规划。
	第t帧观测向量属于状态i转到j的得分=第t-1帧观测向量属于状态i的得分*a[i][j]*b[j](o[t])
	设：第t-1帧观测向量属于状态j的得分=alpha[t-1](i)
	第t帧观测向量属于状态j的得分就是所有状态转到状态j的得分全加，于是有：
	alpha[t](j)=alpha[t-1](1)*a[1][j]*b[j](o[t])+...+alpha[t-1](i)*a[i][j]*b[j](o[t])+...+alpha[t-1](N)*a[N][j]*b[j](o[t])
	上面的式子从2迭代到T
	初始化：alpha[1](j)=初始状态概率*初始状态概率转状态j的概率*b[j](o[1])
	结束：最终得分=alpha[T](1)*a[1][F]+...+alpha[T](s)*a[s][F]+...+alpha[T](N)*a[N][F]
	其中，a[s][F]代表状态s转终末状态F的概率。
3）观测序列解码（Viterbi算法）
	输入：lamda=(A,B)和观测序列O=o[1],o[2],...,o[T]（T是帧数）
	输出：最优状态序列Q=q[1],q[2],...,q[T]
	所谓解码就是把所有的状态序列全部列一遍然后算每种状态序列的得分然后取得分最大的那个序列，但是这样太暴力，于是就有了Viterbi算法，Viterbi算法属于动态规划算法。
	
4）训练（前向后向算法）
5）例子