隐式马尔科夫（HMM）的简单介绍
1）隐式马尔科夫的定义
	状态：Q=q[1],q[2],...,q[N] （ASR中状态就是音素，Part-of-speech tagging中状态就是语法标志）
	状态转移矩阵：A=a[1][1],a[1][2],...,a[N][1],...,a[N][N] （实践中初始值是手动设置的）
	观测向量：O=o[1],o[2],...,o[T] (这里暂时离散化为T种，ASR中观测向量就是MFCC，Part-of-speech tagging中状态就是单词)
	发生概率：B=b[i](o[t]) （这里B用的最多的是混合高斯，用SVM和多层感知的也有很多）
		状态q[i]发生观测o[t]的概率是b[i](o[t])
	开始状态和终结状态的两种表示方式：
		1、开始状态q[0]，和终结状态q[F]
		2、初始状态概率分布和合法接受状态
			初始状态概率分布：pai=pai[1],pai[2],pai[3],...,pai[N]
			合法接受状态是状态Q的子集
	理论上确定一个隐式马尔科夫只需要确定A和B就好所以下面把HMM表示成：
		HMM: lamda=(A,B)
2）观测序列相似度得分（前向算法）
	输入：lamda=(A,B)和观测序列O=o[1],o[2],...,o[T]（T是帧数）
	输出：得分（就是这个观测序列属于lamda的概率）
	观测序列和lamda的相似程度就是对应所有可能的状态序列的概率的全加，但是实践中将所有状态序列全部列出来计算过于暴力，所以有了前向(t-1)算法，前向算法属于动态规划。
	第t帧观测向量属于状态i转到j的得分=第t-1帧观测向量属于状态i的得分*a[i][j]*b[j](o[t])
	设：第t-1帧观测向量属于状态j的得分=alpha[t-1](i)
	第t帧观测向量属于状态j的得分就是所有状态转到状态j的得分全加，于是有：
	alpha[t](j)=alpha[t-1](1)*a[1][j]*b[j](o[t])+...+alpha[t-1](i)*a[i][j]*b[j](o[t])+...+alpha[t-1](N)*a[N][j]*b[j](o[t])
	上面的式子从2迭代到T
	初始化：alpha[1](j)=初始状态概率*初始状态概率转状态j的概率*b[j](o[1])
	终结态：最终得分=alpha[T](1)*a[1][F]+...+alpha[T](s)*a[s][F]+...+alpha[T](N)*a[N][F]  其中，a[s][F]代表状态s转终末状态F的概率。
3）观测序列解码（Viterbi算法）
	输入：lamda=(A,B)和观测序列O=o[1],o[2],...,o[T]（T是帧数）
	输出：最优状态序列Q=q[1],q[2],...,q[T]（以及这个最优状态序列的概率）
	所谓解码就是把所有的状态序列全部列一遍然后算每种状态序列的得分然后取得分最大的那个序列，但是这样太暴力，于是就有了Viterbi算法，Viterbi算法属于动态规划算法。
	设：v[t](j)是t时刻是j状态时最佳状态序列的概率
	所以：v[t](j)=max( v[t-1](1)*a[1][j]*b[j](o[t]),...,v[t-1](i)*a[i][j]*b[j](o[t]),...,v[t-1](N)*a[N][j]*b[j](o[t]) )
	上面的式子从1迭代到T，并记录每一个t对应的j。
	初始化：v[1](j)=初始状态概率*初始状态概率转状态j的概率*b[j](o[1])
	终结态：最终得分=max( v[T](1)*a[1][F]),...,v[T](s)*a[s][F],...,v[T](N)*a[N][F] )  其中，a[s][F]代表状态s转终末状态F的概率
	Viterbi和前向算法是非常相近的，不同点在于前向做Sum Viterbi做Max，还有Viterbi要记录状态序列。
4）训练（前向后向算法）
	输入：观测序列集合、状态集合
	输出：lamda=(A,B)
	下面先计算A：
	根据A的定义有：
	公式1： _a[i][j]=(从状态i转状态j的次数)/(从状态i出发转状态的次数)
	为了从观测序列中统计从状态i转状态j的次数，定义sigma
			sigma[t](i,j)是当观测序列O和lamda作为条件时，t时刻是状态i并且t+1时刻是状态j的概率，也就是下面式子：
	公式2： sigma[t](i,j)=P(q[t]=i,q[t+1]=j | O,lamda)
		观察到O依旧在概率的条件里，所以要把O放出来。下面用一次条件概率就会有：
	公式3： P(q[t]=i,q[t+1]=j | O,lamda)=P(q[t]=i,q[t+1]=j, O| lamda)/P(O|lamda)
			P(q[t]=i,q[t+1]=j, O| lamda)表示lamda条件下观测序列O中，t时刻是状态i并且t+1时刻是状态j的概率
			P(O|lamda)表示lamda发生观测序列O的概率，这里lamda发生观测序列O的概率就是用前向算法算出的相似度得分，也就是：alpha[T](F)
	为了计算lamda条件下i->j在观测序列O中发生的概率，我们需要引入后向概率，计算后向概率的方法叫后向算法（因为前向算法只覆盖到1...t-1部分，所以需要t+1...T才能覆盖住整个O）
	后向概率：
	公式4： beta[t](i)=sum(a[i][1]*b[1](o[t+1])*beta[t+1](1),...,a[i][j]*b[j](o[t+1])*beta[t+1](j),...,a[i][N]*b[N](o[t+1])*beta[t+1](N))
	然后我们就有：
	公式5： P(q[t]=i,q[t+1]=j, O| lamda)=alpha[t](i)*a[i][j]*b[j](o[t+1])*beta[t+1](j)
	带入公式3就可以有：
	公式6： sigma[t](i,j)=( alpha[t](i)*a[i][j]*b[j](o[t+1])*beta[t+1](j) )/(alpha[T](F))
	下面统计从状态i转状态j的次数：
	设从状态i转状态j的次数为SumT[i][j]
	公式7： SumT[i][j]=sum(sigma[1](i,j),   ,sigma[t](i,j),   ,sigma[T-1](i,j))
	下面统计从状态i出发转状态的次数:
	公式8： 从状态i出发转状态的次数=sum(SumT[i][1],...,SumT[i][j],...,SumT[i][N])
	把公式7和公式8带入公式1：
	公式9： _a[i][j]=SumT[i][j]/(SumT[i][1],...,SumT[i][j],...,SumT[i][N])
	下面计算B:
	
5）例子
	（to be continued）
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	